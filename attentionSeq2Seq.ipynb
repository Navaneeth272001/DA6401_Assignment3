{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d26fc8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef3119e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "1. Roman: fiat                  →  Native: ஃபியட்\n",
      "2. Roman: phiyat                →  Native: ஃபியட்\n",
      "3. Roman: piyat                 →  Native: ஃபியட்\n",
      "4. Roman: firaans               →  Native: ஃபிரான்ஸ்\n",
      "5. Roman: france                →  Native: ஃபிரான்ஸ்\n",
      "Dev set\n",
      "1. Roman: fire                  →  Native: ஃபயர்\n",
      "2. Roman: phayar                →  Native: ஃபயர்\n",
      "3. Roman: baar                  →  Native: ஃபார்\n",
      "4. Roman: bar                   →  Native: ஃபார்\n",
      "5. Roman: far                   →  Native: ஃபார்\n"
     ]
    }
   ],
   "source": [
    "#This cell contains necessary code for dataset preprocessing and at the I print few examples for looking how the dataset looks like\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "trainPth = \"/mnt/e_disk/DA6401_Assignment3/dataset/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\"\n",
    "devPth   = \"/mnt/e_disk/DA6401_Assignment3/dataset/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\"\n",
    "testPth = \"/mnt/e_disk/DA6401_Assignment3/dataset/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.test.tsv\"\n",
    "def get_vocab(paths):\n",
    "    chars = set()\n",
    "    for path in paths:\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                native, roman, _ = line.strip().split(\"\\t\")\n",
    "                chars.update(native)\n",
    "                chars.update(roman)\n",
    "    return chars\n",
    "\n",
    "def get_char2idx(char_set):\n",
    "    chars = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"] + sorted(char_set)\n",
    "    return {ch: i for i, ch in enumerate(chars)}, chars\n",
    "\n",
    "\n",
    "\n",
    "char_set = get_vocab([trainPth, devPth])\n",
    "roman2idx, idx2roman = get_char2idx(set(c for c in char_set if c.isascii()))\n",
    "dev2idx, idx2dev = get_char2idx(set(c for c in char_set if not c.isascii()))\n",
    "\n",
    "class TranslitDataset(Dataset):\n",
    "    def __init__(self, path, src_c2i, tgt_c2i, max_len=32):\n",
    "        self.data = []\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                native, roman, _ = line.strip().split(\"\\t\")\n",
    "                self.data.append((roman, native))\n",
    "        self.src_c2i = src_c2i\n",
    "        self.tgt_c2i = tgt_c2i\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        roman, native = self.data[i]\n",
    "        src = [self.src_c2i.get(c, self.src_c2i[\"<unk>\"]) for c in roman[:self.max_len]]\n",
    "        tgt = [self.tgt_c2i[\"<sos>\"]] + \\\n",
    "              [self.tgt_c2i.get(c, self.tgt_c2i[\"<unk>\"]) for c in native[:self.max_len - 1]] + \\\n",
    "              [self.tgt_c2i[\"<eos>\"]]\n",
    "        return torch.tensor(src), torch.tensor(tgt)\n",
    "\n",
    "def pad_batch(batch):\n",
    "    src, tgt = zip(*batch)\n",
    "    src = pad_sequence(src, batch_first=True, padding_value=roman2idx[\"<pad>\"])\n",
    "    tgt = pad_sequence(tgt, batch_first=True, padding_value=dev2idx[\"<pad>\"])\n",
    "    return src, tgt\n",
    "\n",
    "train_ds = TranslitDataset(trainPth, roman2idx, dev2idx, max_len=32)\n",
    "dev_ds   = TranslitDataset(devPth, roman2idx, dev2idx, max_len=32)\n",
    "test_ds   = TranslitDataset(testPth, roman2idx, dev2idx, max_len=32)\n",
    "\n",
    "print(\"Train set\")\n",
    "for i in range(5):\n",
    "    src, tgt = train_ds[i]\n",
    "    roman = ''.join([idx2roman[idx] for idx in src])\n",
    "    native = ''.join([idx2dev[idx] for idx in tgt[1:-1]])  # skip <sos> and <eos>\n",
    "    print(f\"{i+1}. Roman: {roman:20s}  →  Native: {native}\")\n",
    "print(\"Dev set\")\n",
    "for i in range(5):\n",
    "    src, tgt = dev_ds[i]\n",
    "    roman = ''.join([idx2roman[idx] for idx in src])\n",
    "    native = ''.join([idx2dev[idx] for idx in tgt[1:-1]])\n",
    "    print(f\"{i+1}. Roman: {roman:20s}  →  Native: {native}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1988f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Attention Module\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        src_len = encoder_outputs.size(1)\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        return torch.softmax(attention, dim=1)\n",
    "\n",
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, num_layers, cell_type, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.rnn = getattr(nn, cell_type.upper())(emb_dim + hidden_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.output_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        if input.dim() == 1:\n",
    "            input = input.unsqueeze(1)\n",
    "        embedded = self.embedding_dropout(self.embedding(input))\n",
    "\n",
    "        if isinstance(hidden, tuple):\n",
    "            last_hidden = hidden[0][-1]\n",
    "        else:\n",
    "            last_hidden = hidden[-1]\n",
    "\n",
    "        attn_weights = self.attention(last_hidden, encoder_outputs)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n",
    "\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        output = self.output_dropout(output)\n",
    "\n",
    "        prediction = self.fc_out(torch.cat((output.squeeze(1), context.squeeze(1)), dim=1))\n",
    "        return prediction, hidden, attn_weights\n",
    "\n",
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers, cell_type, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.rnn = getattr(nn, cell_type.upper())(emb_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding_dropout(self.embedding(src))\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return outputs, hidden\n",
    "\n",
    "# Seq2Seq Wrapper\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size, trg_len = trg.shape\n",
    "        output_dim = self.decoder.fc_out.out_features\n",
    "        outputs = torch.zeros(batch_size, trg_len, output_dim).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        input = trg[:, 0]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs)\n",
    "            outputs[:, t] = output\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            input = trg[:, t] if teacher_force else output.argmax(1)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "def trim_eos(seq, eos_idx):\n",
    "    seq = seq.tolist()\n",
    "    if eos_idx in seq:\n",
    "        return seq[:seq.index(eos_idx)]\n",
    "    return seq\n",
    "\n",
    "# Modified evaluate_with_attention\n",
    "def evaluate_with_attention(model, input_seq, idx2roman, idx2dev, eos_idx, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(input_seq.unsqueeze(0))  # [1, src_len]\n",
    "        input_token = input_seq[0].view(1)\n",
    "        decoded = []\n",
    "        attentions = []\n",
    "\n",
    "        for _ in range(32):  # max_len\n",
    "            output, hidden, attn = model.decoder(input_token, hidden, encoder_outputs)\n",
    "            top1 = output.argmax(1)\n",
    "            if top1.item() == eos_idx:\n",
    "                break\n",
    "            decoded.append(top1.item())\n",
    "            attentions.append(attn.squeeze(0).cpu())\n",
    "            input_token = top1.unsqueeze(0)\n",
    "\n",
    "    input_tokens = [idx2roman[idx.item()] for idx in input_seq]\n",
    "    output_tokens = [idx2dev[idx] for idx in decoded]\n",
    "\n",
    "    if attentions:\n",
    "        attention_tensor = torch.stack(attentions)  # [output_len, input_len]\n",
    "    else:\n",
    "        attention_tensor = torch.empty(0)\n",
    "\n",
    "    return output_tokens, input_tokens, attention_tensor\n",
    "\n",
    "\n",
    "def calc_word_accuracy(model, loader, eos_idx, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            for i in range(src.size(0)):\n",
    "                pred_seq, _, _ = evaluate_with_attention(model, src[i], idx2roman, idx2dev, eos_idx, device)\n",
    "                true_seq = trim_eos(tgt[i][1:], eos_idx)\n",
    "                pred_idx = [dev2idx[c] for c in pred_seq if c in dev2idx]\n",
    "                if pred_idx == true_seq:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "def train_model(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        wandb.run.name = f\"cell_{config.cell_type}/hid_{config.hidden_dim}/emb_{config.emb_dim}/lay_{config.num_layers}/lr_{config.lr}\"\n",
    "\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True, collate_fn=pad_batch)\n",
    "        dev_loader   = DataLoader(dev_ds, batch_size=config.batch_size, shuffle=False, collate_fn=pad_batch)\n",
    "\n",
    "        vocab_size_input = len(roman2idx)\n",
    "        vocab_size_output = len(dev2idx)\n",
    "\n",
    "        encoder = Encoder(vocab_size_input, config.emb_dim, config.hidden_dim, config.num_layers, config.cell_type, config.dropout)\n",
    "        decoder = Decoder(vocab_size_output, config.emb_dim, config.hidden_dim, config.num_layers, config.cell_type, config.dropout)\n",
    "        model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "        optimizer = opt.Adam(model.parameters(), lr=config.lr)\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=dev2idx[\"<pad>\"])\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        save_path = os.path.join(wandb.run.dir, 'best_model.pth')\n",
    "        eos_idx = dev2idx[\"<eos>\"]\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            token_correct = 0\n",
    "            token_total = 0\n",
    "\n",
    "            for src, tgt in train_loader:\n",
    "                src, tgt = src.to(device), tgt.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(src, tgt, teacher_forcing_ratio=0.5)\n",
    "\n",
    "                output_flat = output[:, 1:].reshape(-1, vocab_size_output)\n",
    "                tgt_flat = tgt[:, 1:].reshape(-1)\n",
    "                loss = criterion(output_flat, tgt_flat)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item() * src.size(0)\n",
    "\n",
    "                preds = output_flat.argmax(1)\n",
    "                mask = tgt_flat != dev2idx[\"<pad>\"]\n",
    "                token_correct += ((preds == tgt_flat) & mask).sum().item()\n",
    "                token_total += mask.sum().item()\n",
    "\n",
    "            train_loss /= len(train_ds)\n",
    "            train_token_acc = token_correct / token_total if token_total > 0 else 0\n",
    "\n",
    "            val_loss = 0\n",
    "            val_token_correct = 0\n",
    "            val_token_total = 0\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for src, tgt in dev_loader:\n",
    "                    src, tgt = src.to(device), tgt.to(device)\n",
    "                    output = model(src, tgt, teacher_forcing_ratio=0.0)\n",
    "\n",
    "                    output_flat = output[:, 1:].reshape(-1, vocab_size_output)\n",
    "                    tgt_flat = tgt[:, 1:].reshape(-1)\n",
    "                    loss = criterion(output_flat, tgt_flat)\n",
    "                    val_loss += loss.item() * src.size(0)\n",
    "\n",
    "                    preds = output_flat.argmax(1)\n",
    "                    mask = tgt_flat != dev2idx[\"<pad>\"]\n",
    "                    val_token_correct += ((preds == tgt_flat) & mask).sum().item()\n",
    "                    val_token_total += mask.sum().item()\n",
    "\n",
    "            val_loss /= len(dev_ds)\n",
    "            val_token_acc = val_token_correct / val_token_total if val_token_total > 0 else 0\n",
    "            val_word_acc = calc_word_accuracy(model, dev_loader, eos_idx, device)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                artifact = wandb.Artifact('best-model', type='model')\n",
    "                artifact.add_file(save_path)\n",
    "                wandb.log_artifact(artifact)\n",
    "\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'train_token_accuracy': train_token_acc,\n",
    "                'val_token_accuracy': val_token_acc,\n",
    "                'val_word_accuracy': val_word_acc\n",
    "            })\n",
    "\n",
    "# Sweep config\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {'name': 'val_loss', 'goal': 'minimize'},\n",
    "    'parameters': {\n",
    "        'epochs': {'values': [10, 15]},\n",
    "        'emb_dim': {'values': [64, 128]},\n",
    "        'hidden_dim': {'values': [128, 256]},\n",
    "        'num_layers': {'values': [1, 2]},\n",
    "        'cell_type': {'values': ['RNN', 'GRU', 'LSTM']},\n",
    "        'lr': {'values': [1e-3, 1e-4]},\n",
    "        'batch_size': {'values': [32, 64]},\n",
    "        'dropout': {'values': [0.2, 0.3]},\n",
    "        'beam_size': {'values': [1, 3, 5]}\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aff47ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ex5t9y6l\n",
      "Sweep URL: https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0ouaa0ll with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \temb_dim: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnavaneeth001\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/e_disk/DA6401_Assignment3/wandb/run-20250520_194327-0ouaa0ll</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/navaneeth001/Assignment3_attention/runs/0ouaa0ll' target=\"_blank\">stoic-sweep-1</a></strong> to <a href='https://wandb.ai/navaneeth001/Assignment3_attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/navaneeth001/Assignment3_attention' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/navaneeth001/Assignment3_attention/runs/0ouaa0ll' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention/runs/0ouaa0ll</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_token_accuracy</td><td>▁▅▆▇▇▇█████████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_token_accuracy</td><td>▁▅▇▇▇▇█████████</td></tr><tr><td>val_word_accuracy</td><td>▁▄▅▆▆▇▇▇▇██████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_loss</td><td>0.44512</td></tr><tr><td>train_token_accuracy</td><td>0.88396</td></tr><tr><td>val_loss</td><td>0.69121</td></tr><tr><td>val_token_accuracy</td><td>0.84968</td></tr><tr><td>val_word_accuracy</td><td>0.50696</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cell_GRU/hid_128/emb_64/lay_2/lr_0.0001</strong> at: <a href='https://wandb.ai/navaneeth001/Assignment3_attention/runs/0ouaa0ll' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention/runs/0ouaa0ll</a><br> View project at: <a href='https://wandb.ai/navaneeth001/Assignment3_attention' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention</a><br>Synced 5 W&B file(s), 0 media file(s), 26 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_194327-0ouaa0ll/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cwozmg1s with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \temb_dim: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/e_disk/DA6401_Assignment3/wandb/run-20250520_201348-cwozmg1s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/navaneeth001/Assignment3_attention/runs/cwozmg1s' target=\"_blank\">colorful-sweep-2</a></strong> to <a href='https://wandb.ai/navaneeth001/Assignment3_attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/navaneeth001/Assignment3_attention' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/navaneeth001/Assignment3_attention/runs/cwozmg1s' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention/runs/cwozmg1s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▆▄▃▂▂▁▁▁▁</td></tr><tr><td>train_token_accuracy</td><td>▁▃▅▆▇▇████</td></tr><tr><td>val_loss</td><td>█▆▄▃▂▂▁▁▁▁</td></tr><tr><td>val_token_accuracy</td><td>▁▃▅▆▇▇████</td></tr><tr><td>val_word_accuracy</td><td>▁▁▃▅▅▆▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss</td><td>0.60541</td></tr><tr><td>train_token_accuracy</td><td>0.84388</td></tr><tr><td>val_loss</td><td>0.81119</td></tr><tr><td>val_token_accuracy</td><td>0.81658</td></tr><tr><td>val_word_accuracy</td><td>0.40032</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cell_LSTM/hid_128/emb_64/lay_1/lr_0.0001</strong> at: <a href='https://wandb.ai/navaneeth001/Assignment3_attention/runs/cwozmg1s' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention/runs/cwozmg1s</a><br> View project at: <a href='https://wandb.ai/navaneeth001/Assignment3_attention' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention</a><br>Synced 5 W&B file(s), 0 media file(s), 20 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_201348-cwozmg1s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j5dpo5ji with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \temb_dim: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/e_disk/DA6401_Assignment3/wandb/run-20250520_202737-j5dpo5ji</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/navaneeth001/Assignment3_attention/runs/j5dpo5ji' target=\"_blank\">hardy-sweep-3</a></strong> to <a href='https://wandb.ai/navaneeth001/Assignment3_attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/navaneeth001/Assignment3_attention' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/navaneeth001/Assignment3_attention/runs/j5dpo5ji' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention/runs/j5dpo5ji</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>train_token_accuracy</td><td>▁▆▇▇▇█████</td></tr><tr><td>val_loss</td><td>█▄▄▄▁▁▃▂▃▃</td></tr><tr><td>val_token_accuracy</td><td>▁▄▆▆██▇▇█▇</td></tr><tr><td>val_word_accuracy</td><td>▁▄▇▅▆██▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss</td><td>0.4025</td></tr><tr><td>train_token_accuracy</td><td>0.89496</td></tr><tr><td>val_loss</td><td>0.75927</td></tr><tr><td>val_token_accuracy</td><td>0.8386</td></tr><tr><td>val_word_accuracy</td><td>0.41277</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cell_RNN/hid_128/emb_128/lay_1/lr_0.001</strong> at: <a href='https://wandb.ai/navaneeth001/Assignment3_attention/runs/j5dpo5ji' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention/runs/j5dpo5ji</a><br> View project at: <a href='https://wandb.ai/navaneeth001/Assignment3_attention' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_202737-j5dpo5ji/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 687e9gyx with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \temb_dim: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/e_disk/DA6401_Assignment3/wandb/run-20250520_204605-687e9gyx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/navaneeth001/Assignment3_attention/runs/687e9gyx' target=\"_blank\">exalted-sweep-4</a></strong> to <a href='https://wandb.ai/navaneeth001/Assignment3_attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/navaneeth001/Assignment3_attention' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/navaneeth001/Assignment3_attention/runs/687e9gyx' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention/runs/687e9gyx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▅▃▂▂▁▁▁▁▁</td></tr><tr><td>train_token_accuracy</td><td>▁▄▆▇▇█████</td></tr><tr><td>val_loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>val_token_accuracy</td><td>▁▄▅▆▇█▇███</td></tr><tr><td>val_word_accuracy</td><td>▁▂▃▅▆▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss</td><td>0.64609</td></tr><tr><td>train_token_accuracy</td><td>0.83026</td></tr><tr><td>val_loss</td><td>0.81307</td></tr><tr><td>val_token_accuracy</td><td>0.81546</td></tr><tr><td>val_word_accuracy</td><td>0.41643</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cell_RNN/hid_128/emb_64/lay_1/lr_0.0001</strong> at: <a href='https://wandb.ai/navaneeth001/Assignment3_attention/runs/687e9gyx' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention/runs/687e9gyx</a><br> View project at: <a href='https://wandb.ai/navaneeth001/Assignment3_attention' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention</a><br>Synced 5 W&B file(s), 0 media file(s), 18 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_204605-687e9gyx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y2slk2ty with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \temb_dim: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/e_disk/DA6401_Assignment3/wandb/run-20250520_205915-y2slk2ty</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/navaneeth001/Assignment3_attention/runs/y2slk2ty' target=\"_blank\">sweet-sweep-5</a></strong> to <a href='https://wandb.ai/navaneeth001/Assignment3_attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/navaneeth001/Assignment3_attention' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/navaneeth001/Assignment3_attention/runs/y2slk2ty' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention/runs/y2slk2ty</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_token_accuracy</td><td>▁▆▇▇▇▇▇████████</td></tr><tr><td>val_loss</td><td>█▄▄▂▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>val_token_accuracy</td><td>▁▅▅▇▇▇██▇██████</td></tr><tr><td>val_word_accuracy</td><td>▁▄▄▆▆▇▇▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_loss</td><td>0.31577</td></tr><tr><td>train_token_accuracy</td><td>0.9176</td></tr><tr><td>val_loss</td><td>0.6495</td></tr><tr><td>val_token_accuracy</td><td>0.86141</td></tr><tr><td>val_word_accuracy</td><td>0.57068</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cell_GRU/hid_256/emb_64/lay_2/lr_0.0001</strong> at: <a href='https://wandb.ai/navaneeth001/Assignment3_attention/runs/y2slk2ty' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention/runs/y2slk2ty</a><br> View project at: <a href='https://wandb.ai/navaneeth001/Assignment3_attention' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention</a><br>Synced 5 W&B file(s), 0 media file(s), 18 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_205915-y2slk2ty/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# wandb sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project='Assignment3_attention')\n",
    "wandb.agent(sweep_id, function=train_model, count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8363ec66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'Assignment3_attention' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'navaneeth001' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/e_disk/DA6401_Assignment3/wandb/run-20250520_215517-y2slk2ty</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/navaneeth001/Assignment3_attention/runs/y2slk2ty' target=\"_blank\">cell_GRU/hid_256/emb_64/lay_2/lr_0.0001</a></strong> to <a href='https://wandb.ai/navaneeth001/Assignment3_attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/navaneeth001/Assignment3_attention' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention/sweeps/ex5t9y6l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/navaneeth001/Assignment3_attention/runs/y2slk2ty' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention/runs/y2slk2ty</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model artifact to: /mnt/e_disk/DA6401_Assignment3/artifacts/best-model:v67/best_model.pth\n",
      "\n",
      "Test Token Accuracy: 0.8431\n",
      "Test Word Accuracy:  0.5479\n",
      "Saved predictions to /mnt/e_disk/DA6401_Assignment3/predictions_attention/test_predictions.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_token_accuracy</td><td>▁</td></tr><tr><td>test_word_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_token_accuracy</td><td>0.84314</td></tr><tr><td>test_word_accuracy</td><td>0.54793</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cell_GRU/hid_256/emb_64/lay_2/lr_0.0001</strong> at: <a href='https://wandb.ai/navaneeth001/Assignment3_attention/runs/y2slk2ty' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention/runs/y2slk2ty</a><br> View project at: <a href='https://wandb.ai/navaneeth001/Assignment3_attention' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment3_attention</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_215517-y2slk2ty/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import wandb\n",
    "from wandb import Api\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Load best model config from sweep ===\n",
    "ENTITY     = 'navaneeth001'\n",
    "PROJECT    = 'Assignment3_attention'\n",
    "SWEEP_ID   = 'ex5t9y6l'\n",
    "ARTIFACT_REF = 'navaneeth001/Assignment3_attention/best-model:v67'\n",
    "OUTPUT_DIR = '/mnt/e_disk/DA6401_Assignment3/predictions_attention'\n",
    "CSV_PATH   = os.path.join(OUTPUT_DIR, 'test_predictions.csv')\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "pred_rows = [[\"Input\", \"Target\", \"Prediction\"]]\n",
    "\n",
    "api      = Api()\n",
    "sweep    = api.sweep(f\"{ENTITY}/{PROJECT}/{SWEEP_ID}\")\n",
    "runs     = sweep.runs\n",
    "best_run = max(runs, key=lambda r: r.summary.get('val_word_accuracy', float('inf')))\n",
    "cfg      = best_run.config\n",
    "\n",
    "eval_run = wandb.init(\n",
    "    project=PROJECT,\n",
    "    entity=ENTITY,\n",
    "    job_type='evaluation'\n",
    ")\n",
    "artifact     = eval_run.use_artifact(ARTIFACT_REF, type='model')\n",
    "download_dir = artifact.download()\n",
    "\n",
    "model_path   = '/mnt/e_disk/DA6401_Assignment3/artifacts/best-model:v67/best_model.pth'\n",
    "print(f\"Loaded model artifact to: {model_path}\")\n",
    "\n",
    "# === Load model ===\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def evaluate_with_attention(model, input_seq, idx2roman, idx2dev, eos_idx, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(input_seq.unsqueeze(0))\n",
    "        input_token = input_seq[0].view(1)\n",
    "        decoded = []\n",
    "        attentions = []\n",
    "\n",
    "        for _ in range(32):  # max_len\n",
    "            output, hidden, attn = model.decoder(input_token, hidden, encoder_outputs)\n",
    "            top1 = output.argmax(1)\n",
    "            if top1.item() == eos_idx:\n",
    "                break\n",
    "            decoded.append(top1.item())\n",
    "            attentions.append(attn.squeeze(0).cpu())\n",
    "            input_token = top1.unsqueeze(0)\n",
    "\n",
    "    input_tokens = [idx2roman[idx.item()] for idx in input_seq]\n",
    "    output_tokens = [idx2dev[idx] for idx in decoded]\n",
    "\n",
    "    if attentions:\n",
    "        attention_tensor = torch.stack(attentions)\n",
    "    else:\n",
    "        attention_tensor = torch.empty(0)\n",
    "\n",
    "    return output_tokens, input_tokens, attention_tensor\n",
    "\n",
    "def test_model(model_path, cfg, device, pred_rows):\n",
    "    test_loader = DataLoader(test_ds, batch_size=64, shuffle=False, collate_fn=pad_batch)\n",
    "\n",
    "    vocab_size_input = len(roman2idx)\n",
    "    vocab_size_output = len(dev2idx)\n",
    "\n",
    "    encoder = Encoder(\n",
    "        vocab_size_input,\n",
    "        64,\n",
    "        256,\n",
    "        2,\n",
    "        \"GRU\",\n",
    "        0.3\n",
    "    )\n",
    "    decoder = Decoder(\n",
    "        vocab_size_output,\n",
    "        64,\n",
    "        256,\n",
    "        2,\n",
    "        \"GRU\",\n",
    "        0.3\n",
    "    )\n",
    "\n",
    "    model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    eos_idx = dev2idx[\"<eos>\"]\n",
    "    total_token_correct = 0\n",
    "    total_token_count = 0\n",
    "    total_word_correct = 0\n",
    "    total_word_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in test_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            for i in range(src.size(0)):\n",
    "                input_seq = src[i]\n",
    "                true_seq = trim_eos(tgt[i][1:], eos_idx)\n",
    "\n",
    "                pred_tokens, input_tokens, attn_weights = evaluate_with_attention(\n",
    "                    model, input_seq, idx2roman, idx2dev, eos_idx, device\n",
    "                )\n",
    "                pred_idx = [dev2idx[token] for token in pred_tokens if token in dev2idx]\n",
    "\n",
    "                # Token Accuracy\n",
    "                match_length = min(len(pred_idx), len(true_seq))\n",
    "                for j in range(match_length):\n",
    "                    if pred_idx[j] == true_seq[j]:\n",
    "                        total_token_correct += 1\n",
    "                total_token_count += len(true_seq)\n",
    "\n",
    "                # Word Accuracy\n",
    "                if pred_idx == true_seq:\n",
    "                    total_word_correct += 1\n",
    "                total_word_count += 1\n",
    "\n",
    "                # Save row to CSV\n",
    "                pred_rows.append([\n",
    "                    \"\".join(input_tokens),\n",
    "                    \"\".join([idx2dev[int(i)] for i in true_seq]),\n",
    "                    \"\".join(pred_tokens)\n",
    "                ])\n",
    "\n",
    "    token_accuracy = total_token_correct / total_token_count if total_token_count > 0 else 0\n",
    "    word_accuracy = total_word_correct / total_word_count if total_word_count > 0 else 0\n",
    "\n",
    "    print(f\"\\nTest Token Accuracy: {token_accuracy:.4f}\")\n",
    "    print(f\"Test Word Accuracy:  {word_accuracy:.4f}\")\n",
    "\n",
    "    wandb.log({\n",
    "        'test_token_accuracy': token_accuracy,\n",
    "        'test_word_accuracy': word_accuracy\n",
    "    })\n",
    "\n",
    "# === Run test and collect predictions ===\n",
    "test_model(model_path, cfg, device, pred_rows)\n",
    "\n",
    "# === Save predictions to CSV ===\n",
    "with open(CSV_PATH, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(pred_rows)\n",
    "\n",
    "print(f\"Saved predictions to {CSV_PATH}\")\n",
    "\n",
    "eval_run.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b05203",
   "metadata": {},
   "source": [
    "# Question 6 (20 Marks)\n",
    "\n",
    "This a challenge question and most of you will find it hard. \n",
    "\n",
    "I like the visualisation in the figure captioned \"Connectivity\" in this [article](https://distill.pub/2019/memorization-in-rnns/#appendix-autocomplete). Make a similar visualisation for your model. Please look at this [blog](https://medium.com/data-science/visualising-lstm-activations-in-keras-b50206da96ff) for some starter code. The goal is to figure out the following: When the model is decoding the $i$-th character in the output which is the input character that it is looking at?\n",
    "\n",
    "Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba57419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attention-heatmaps</strong> at: <a href='https://wandb.ai/navaneeth001/attention-visualization/runs/mtx4o91p' target=\"_blank\">https://wandb.ai/navaneeth001/attention-visualization/runs/mtx4o91p</a><br> View project at: <a href='https://wandb.ai/navaneeth001/attention-visualization' target=\"_blank\">https://wandb.ai/navaneeth001/attention-visualization</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_192435-mtx4o91p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/e_disk/DA6401_Assignment3/wandb/run-20250520_193546-dfs4nvrm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/navaneeth001/attention-visualization/runs/dfs4nvrm' target=\"_blank\">attention-heatmaps</a></strong> to <a href='https://wandb.ai/navaneeth001/attention-visualization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/navaneeth001/attention-visualization' target=\"_blank\">https://wandb.ai/navaneeth001/attention-visualization</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/navaneeth001/attention-visualization/runs/dfs4nvrm' target=\"_blank\">https://wandb.ai/navaneeth001/attention-visualization/runs/dfs4nvrm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f860e8206ff144a193703ca885fa8f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Example:', layout=Layout(width='80%'), options=('0: tensor([ 9,  4,  4, 21, 16])', '1: t…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_15.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_15.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import ipywidgets as widgets\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "import plotly.io as pio\n",
    "import wandb\n",
    "\n",
    "# Set Plotly renderer suitable for notebooks\n",
    "pio.renderers.default = \"iframe\"\n",
    "\n",
    "# Initialize Weights & Biases (use your project name & reinit=True to allow multiple runs)\n",
    "wandb.init(project=\"attention-visualization\", name=\"attention-heatmaps\", reinit=True)\n",
    "\n",
    "# ---------- Attention heatmap plotting ----------\n",
    "def plot_attention_heatmap_plotly(attn_weights, input_chars, output_chars, log_to_wandb=False, step=None):\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=attn_weights,\n",
    "        x=input_chars,\n",
    "        y=output_chars,\n",
    "        colorscale='Viridis',\n",
    "        hoverongaps=False,\n",
    "        hovertemplate='<b>Output:</b> %{y}<br><b>Input:</b> %{x}<br><b>Attention:</b> %{z:.3f}<extra></extra>',\n",
    "        colorbar=dict(title='Attention Weight')\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f'Attention Heatmap (Step {step})' if step is not None else 'Attention Heatmap',\n",
    "        xaxis_title='Input (Romanized)',\n",
    "        yaxis_title='Output (Native)',\n",
    "        font=dict(family=\"Noto Sans\", size=14),\n",
    "        autosize=True,\n",
    "        margin=dict(l=50, r=50, t=50, b=50)\n",
    "    )\n",
    "\n",
    "    if log_to_wandb:\n",
    "        wandb.log({f\"attention_heatmap_{step or 0}\": wandb.Plotly(fig)})\n",
    "\n",
    "    fig.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# ---------- Interactive attention visualizer ----------\n",
    "def visualize_attention(model, dataset, idx2src, idx2tgt, eos_idx, device):\n",
    "    options = [f\"{i}: {''.join([idx2src[idx.item()] for idx in dataset[i][0]])}\" for i in range(len(dataset))]\n",
    "    dropdown = widgets.Dropdown(options=options[:100], description=\"Example:\", layout=widgets.Layout(width='80%'))\n",
    "\n",
    "    def on_change(change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            i = int(change['new'].split(\":\")[0])\n",
    "            src, tgt = dataset[i]\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "\n",
    "            pred_chars, input_chars, attn_tensor = evaluate_with_attention(\n",
    "                model, src, idx2src, idx2tgt, eos_idx, device\n",
    "            )\n",
    "\n",
    "            if attn_tensor.nelement() == 0:\n",
    "                print(f\"⚠️ Empty attention weights for example {i}. Possibly predicted <eos> immediately.\")\n",
    "                print(f\"Input:  {''.join(input_chars)}\")\n",
    "                print(f\"Target: {''.join([idx2tgt[idx.item()] for idx in tgt[1:-1]])}\")\n",
    "                print(f\"Pred:   {''.join(pred_chars)}\")\n",
    "                return\n",
    "\n",
    "            attn_matrix = attn_tensor.cpu().numpy()  # To NumPy for plotly\n",
    "            plot_attention_heatmap_plotly(attn_matrix, input_chars, pred_chars, log_to_wandb=True, step=i)\n",
    "\n",
    "            print(\"\\n📝 Sequence Info:\")\n",
    "            print(f\"Input:  {''.join(input_chars)}\")\n",
    "            print(f\"Target: {''.join([idx2tgt[idx.item()] for idx in tgt[1:-1]])}\")\n",
    "            print(f\"Pred:   {''.join(pred_chars)}\")\n",
    "\n",
    "    dropdown.observe(on_change)\n",
    "    display(dropdown)\n",
    "\n",
    "\n",
    "# -------------- Usage example -----------------\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Make sure these are consistent with your training config:\n",
    "encoder = Encoder(len(roman2idx), 128, 256, 1, \"LSTM\", 0.2)\n",
    "decoder = Decoder(len(dev2idx), 128, 256, 1, \"LSTM\", 0.2)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Load your trained weights (replace path with your model's path)\n",
    "model.load_state_dict(torch.load(\"path_to_best_model.pth\", map_location=device))\n",
    "\n",
    "eos_idx = dev2idx[\"<eos>\"]\n",
    "\n",
    "# Call the interactive visualizer on your test dataset\n",
    "visualize_attention(model, test_ds, idx2roman, idx2dev, eos_idx, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8907d047",
   "metadata": {},
   "source": [
    "Best model configs obtained from sweeps\n",
    "\n",
    "- batch_size - 32\n",
    "- beam_size - 5\n",
    "- cell_type - \"LSTM\"\n",
    "- dropout - 0.2\n",
    "- emb_dim - 128\n",
    "- epochs - 10\n",
    "- hidden_dim - 256\n",
    "- lr - 0.0001\n",
    "- num_layers - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ef35db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_input = len(roman2idx)\n",
    "vocab_size_output = len(dev2idx)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "encoder = Encoder(vocab_size_input, 128, 256, 1, \"LSTM\", 0.2)\n",
    "decoder = Decoder(vocab_size_output, 128, 256, 1, \"LSTM\", 0.2)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7b26e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990705fd39e54b52a527ea334a2ccd3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Example:', layout=Layout(width='80%'), options=('0: tensor([ 9,  4,  4, 21, 16])', '1: t…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_11.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "eos_idx = dev2idx[\"<eos>\"]\n",
    "visualize_attention(model, test_ds, idx2roman, idx2dev, eos_idx, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da24s008",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
